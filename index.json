[{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/autodiff/","section":"Tags","summary":"","title":"Autodiff","type":"tags"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/","section":"Carlos's Website","summary":"","title":"Carlos's Website","type":"page"},{"content":" Classical mechanics of particles is a subject which serves as gateway to concepts, tools and techniques which are common currency in every field which aims to describe dynamical phenomena, as well as in contemporary data science and machine learning which rely on finding solutions of optimization problems. An example of a classical optimization problem is that of the motion of a particle subjected to constraints due to a physical barrier (see the movie below), where a mass under the action of gravity moves over a surface.\nSolution obtained with the implementation described in this post. In this example, a particle with an initial potential energy moves over the surface of a paraboloid. The code to solve this type of problems can be found here. In this post I discuss how to solve the type of problem mentioned above using the Lagrange multipliers method. First, I briefly present the maths and then I describe how to use automatic differentiation to numerically solve the equations using the Runge-Kutta method. As with almost every post, I also share an interactive implementation of the method. On this occasion, It is built using threejs and pyodide.\nInteractive Demo: # Online implementation using Pyodide and Threejs. Of course, it works better on a tab of its own! Below I describe some details on how to make your own. For best result, make sure to check the Pyodide documentation. In this demo, once you have set the initial position xo and yo and the surface with parameters A,B. click on the Run button and wait until the Plot is enabled to plot the trajectory!. You can move the scene around and zoom, with the mouse or touch gestures. Mechanics of a particle with constraints: The Lagrange multipliers method. # In rectangular coordinates \\( r=(x,y,z )\\) the motion of a particle of mass \\( m \\) under the effect of gravity has a Lagrangian function: $$ L_o = \\frac{1}{2} m \\dot{r}^2 + m g z \\quad(1)$$\nIf the particle is constrained to move over a surface such an inclined plane or another landscape which can be described by a function \\( z = f(x,y) \\). The Lagrangian \\( (1) \\) needs to be complemented with a contribution for the constraint as:\n$$L = L_o + \\lambda(z-f(x,y)) \\quad(2)$$\nWhere \\( \\lambda \\) is a constant called the lagrange multiplier, which needs to be determined to ensure the constrained is fulfilled. The equations of motion obtained by optimizing \\( (2) \\) are: $$m\\ddot{x} = -\\lambda f_x\\quad(3)$$ $$m\\ddot{y} = -\\lambda f_y \\quad(4)$$ $$m\\ddot{z} = -mg + \\lambda\\quad(5)$$\nWhere the indices in \\( f\\) stand for the partial derivative with respect to that variable. The system \\( (3)-(5)\\) needs to be complemented with the constraint equation obtained differentiating \\( z \\) respect to time twice:\n$$\\ddot{z} =(\\dot{x}^2 f_{xx} + \\dot{y}^2f_{yy})+\\dot{x}\\dot{y}(f_{xy}+f_{yx})+(\\ddot{x}f_x+\\dot{y}f_y)\\quad(6)$$\nSolving for \\( \\lambda \\) using \\( (3)-(6)\\) gives:\n$$\\lambda = \\frac{m}{1+f_x^2+f_y^2}(g + \\dot{x}^2f_{xx} + \\dot{y}^2f_{yy} + \\dot{x}\\dot{y}(f_{xy}+f_{yx}))\\quad(7) $$\nWhich leads to the equations of motion:\n$$\\ddot{x}=-\\frac{g + \\dot{x}^2f_{xx} + \\dot{y}^2f_{yy} + \\dot{x}\\dot{y}(f_{xy}+f_{yx}) }{1+f_x^2+f_y^2} f_x \\quad(8)$$ $$\\ddot{y}=-\\frac{g+ \\dot{x}^2f_{xx} + \\dot{y}^2f_{yy} + \\dot{x}\\dot{y}(f_{xy}+f_{yx})}{1+f_x^2+f_y^2} f_y \\quad(9)$$ $$\\ddot{z}=-g + \\frac{g+\\dot{x}^2f_{xx} + \\dot{y}^2f_{yy} + \\dot{x}\\dot{y}(f_{xy}+f_{yx})}{1+f_x^2+f_y^2} \\quad(10)$$\nThere is a lot that can be said about these equations, which is of course beyond the scope of this little blog, however, it is worth noticing that in general these constitute a nonlinear system, which can\u0026rsquo;t be solved analytically, except for a few simple cases, such as an inclined plane. The system can be numerically solved using a general integration scheme such as the Runge-Kutta method, provided we can efficiently compute the derivatives of the constraint.\nRunge-Kutta integration. # The numerical solution of a system of the form \\(\\dot{ \\bf{x} }= \\bf{F(x,t)} \\) is obtained by:\n$${\\bf{x}}_{n+1} = {\\bf{x}}_n + (1/6)( {\\bf{k}}_1 + 2 {\\bf{k}}_2 + 2 {\\bf{k}}_3 + {\\bf{k}}_4 )\\quad(11)$$\n$$ {\\bf{k}}_1 = {\\bf{F(x }}_n,t ) \\quad(12)$$ $$ {\\bf{k}}_2 = {\\bf{F(x }}_n + (h/2){\\bf{k}}_1 ,t+h/2)\\quad(13) $$ $$ {\\bf{k}}_3 = {\\bf{F(x }}_n + (h/2){\\bf{k}}_2,t+h/2)\\quad(14) $$ $$ {\\bf{k}}_4 = {\\bf{F(x }}_n + h{\\bf{k}}_3,t+h)\\quad(15) $$\nWhere \\( h \\) is the integration step. This is a fourth order method and it requires to evaluate the function \\( \\bf{F} \\) four times per iteration. In this case: $$ {\\bf{x}} = (x, y, z, \\dot{x}, \\dot{y},\\dot{z}) \\quad(16)$$\nAnd the Function \\( \\bf{F} \\): $${\\bf{F}} = (\\dot{x},\\dot{y},\\dot{z},\\ddot{x},\\ddot{y},\\dot{z})\\quad(17) $$ where the last three entries are given by \\( (8)-(10) \\).\nWith everything defined then it is very simple to write a program to solve the problem as long as we know the partial derivatives of the constraint. This is where automatic differentiation shines!.\nAutomatic differentiation. # The numerical solution of optimization problems is required everywhere, as its is a fundamental component to machine learning and A.I. implementations. Accordingly, the availability of tools which automate the task of computing gradients has also evolved and are now available in a number of frameworks, such as JAX and Pytorch for the python language. In what follows I will be using Autograd, as it is well mantained, documented and offers the versatility of using numpy and can be readily used in webassembly projects.\nEnter Autograd. # Let us consider a set of functions which depend of \\( {(x,y)} \\) and a couple of parameters \\( {(A,B).} \\ \\ \\) For example:\ndef plane(a,b,x,y): return a*x + b*y def paraboloid(a,b,x,y): return a*x*x + b*y*y def wave(a,b,x,y): return 0.2*(x*x+y*y)*np.cos(a*x*x+b*y*y) Then it is easy to write down the function \\( \\bf{F} \\) in \\( (17) \\) as:\ndef kn(r,pars,case): a = pars[0] b = pars[1] g = 10 if case in [\u0026#39;plane\u0026#39;,\u0026#39;paraboloid\u0026#39;,\u0026#39;wave\u0026#39;]: if case == \u0026#39;plane\u0026#39;: fn = plane if case == \u0026#39;paraboloid\u0026#39;: fn = paraboloid if case == \u0026#39;wave\u0026#39;: fn = wave x = r[0]; y = r[1]; z = r[2] vx = r[3]; vy =r[4]; vz= r[5] fx = grad(fn,2)(a,b,x,y) fy = grad(fn,3)(a,b,x,y) dg = np.array([fx,fy,1]) dg2 = np.dot(dg,dg) fxx = grad(grad(fn,2),2)(a,b,x,y) fyy = grad(grad(fn,3),3)(a,b,x,y) fxy = grad(grad(fn,2),3)(a,b,x,y) fyx = grad(grad(fn,3),2)(a,b,x,y) ddg = np.array([fxx*vx*vx,fyy*vy*vy,g,vx*vy*(fxy+fyx)]) Num = np.sum(ddg) Den = dg2 A = Num/dg2 F = np.array([vx,vy,vz,-A*fx,-A*fy,-g+A]) return F The function objects depend of four variables, the two parameters \\(a,b \\) and the coordinates \\( x,y \\). To obtain the partial derivatives with respect to the variables we want, grad takes the index of the variable defined in the function. In the cases above \\( x \\) and \\( y \\) correspond to the second and third indices. Then, the required derivatives are simply computed as:\nfx = grad(fn,2)(a,b,x,y) fy = grad(fn,3)(a,b,x,y) fxx = grad(grad(fn,2),2)(a,b,x,y) fyy = grad(grad(fn,3),3)(a,b,x,y) fxy = grad(grad(fn,2),3)(a,b,x,y) fyx = grad(grad(fn,3),2)(a,b,x,y) And the Runge-Kutta loop as:\ndef RK(tf,ro,case,h,a,b): dtp = 1e-3 t = 0.0; u = np.zeros(6) u[0:3] = ro prm = [a,b] while t\u0026lt;tf: k1 = kn(u,prm,case) k2 = kn(u+0.5*h*k1,prm,case) k3 = kn(u+0.5*h*k2,prm,case) k4 = kn(u+h*k3,prm,case) u = u+ (h/6.0)*(k1+2*k2+2*k3+k4) t+=h return u Webassembly Implementation. # This project was made with the excuse of developing the numerical work and the visualisation independently, and being able to share it on the blog, taking advantage of the lovely threejs library. To achieve this, the set of functions in a given file, can be accessed by the javascript scope as follows: Import the Pyodide javascript module in the head of your document: \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/pyodide/v0.27.5/full/pyodide.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; If the numerical methods are located all in a file named, lets say, \u0026ldquo;integrals.py\u0026rdquo;. Then, the methods are made accessible to javascript in the initialisation part with something like this: //Load Pyodide and install the autograd package. const pyodideRuntime = await loadPyodide(); await pyodideRuntime.loadPackage(\u0026#34;micropip\u0026#34;); const micropip = pyodideRuntime.pyimport(\u0026#34;micropip\u0026#34;); await micropip.install(\u0026#34;autograd\u0026#34;) //Access the methods in \u0026#34;integrals.py\u0026#34;. pyodideRuntime.runPython(await (await fetch(\u0026#34;integrals.py\u0026#34;)).text()); //Registers the functions used in the three.js script. These can now be used as any JS function. let pyparaboloid = pyodideRuntime.globals.get(\u0026#39;paraboloid\u0026#39;); let pyplane = pyodideRuntime.globals.get(\u0026#39;plane\u0026#39;); let pywave = pyodideRuntime.globals.get(\u0026#39;wave\u0026#39;); let RK = pyodideRuntime.globals.get(\u0026#39;RK\u0026#39;); That is all for now! The repository with the code to integrate, as well at the code to pu the simulation on line is available here. The live demo can be found here. Until next time!\n","date":"24 April 2025","externalUrl":null,"permalink":"/posts/constraints-in-mechanics-and-automatic-differentiation./","section":"Posts","summary":"","title":"Lagrange Multipliers, Mechanics and Automatic Differentiation.","type":"posts"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/mechanics/","section":"Tags","summary":"","title":"Mechanics","type":"tags"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/pyodide/","section":"Tags","summary":"","title":"Pyodide","type":"tags"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/threejs/","section":"Tags","summary":"","title":"Threejs","type":"tags"},{"content":"","date":"24 April 2025","externalUrl":null,"permalink":"/tags/webgl/","section":"Tags","summary":"","title":"WebGL","type":"tags"},{"content":"","date":"23 January 2025","externalUrl":null,"permalink":"/tags/complex-systems/","section":"Tags","summary":"","title":"Complex Systems","type":"tags"},{"content":"","date":"23 January 2025","externalUrl":null,"permalink":"/tags/game-theory/","section":"Tags","summary":"","title":"Game Theory","type":"tags"},{"content":"","date":"23 January 2025","externalUrl":null,"permalink":"/tags/learning-and-evolution/","section":"Tags","summary":"","title":"Learning and Evolution","type":"tags"},{"content":" The surge of technologies like WebAssembly and its associated ecosystem ecmscripten , pyscript and panel, allow the development of web content in languages like C, Go, Julia or Python, which can be executed on almost any modern web-browser. This provided me with an excellent justification to revise some interesting models and simulations close to my heart, and which can be used to produce demos I can put on spaces like this website.\nAn example, very much of interest today, not only to me but to several people in the ecology and evolution communities is the formation, co-evolution and resilience of ecological networks. In this post I briefly present a very popular model of evolutionary dynamics, the web world model, and present a python implementation of it, which I also embed on the page.\nThe advantage of being able to share codes in repositories for anybody to run their own simulations, is now enhanced with the ability to literally execute the same code in the browser! This allows anybody to get a flavour of the dynamics without having to install and download anything.\nFoodwebs and ecological evolution. # Before discussing the main features of the webworld model. Let me introduce the concept of foodweb. In a nutshell, foodwebs are the network-like hierarchical structures which result of quantifying which species eats which species in a given ecosystem. Typical foodweb evolution obtained with webworld. Each node represents a species, the node radius is equal to the logarithm of the population and the width of the links is equal to the strength of the interaction. We start with a single species, and continue to show the network as the system evolves by adding more species. The arrows point in the direction of the flow of resources. At the very beginning of the evolution, the only species present feeds from the external resources. A foodweb is constructed by considering all the species present in an ecosystem and quantifying all the predators and prey of each species. If a species does not posses any prey, but the external resources, then it belongs to the basal trophic level. If on the contrary, a species does not have any predators, only prey, then it belongs to the top trophic level. Species predator and prey species are accommodated in a trophic level computed by the shortest path to the external resources. In general, foodwebs only have a few of trophic levels and the population sizes of basal species are often orders of magnitude larger that those is the intermediate and upper levels. The webworld model - evolution as a complex system. # A very important aspect of modelling natural phenomena consists of linking the set of equations and core ideas with some representation of the solutions, like a plot or a movie of showing the dynamical behaviour of the system, which is not an easy task itself. Specially, If the system under study is operates in several time-scales in space and time, and it is composed with many different interacting elements. Complex systems, are those in which their components interact locally amongst them according to some set of rules and are unaware of any global pattern emerging from those interactions. For instance, in an ecosystem, species interact by predator/prey relationships. Some species prey on other species, and at the same time are eaten by some predator species, alongside those processes, species also produce offspring an die of natural causes with some seasonal rate, thus, modifying the species population. The population update processes, might lead to patterns, such as oscillatory behaviours, stable co-existence and extinctions, events which occur at the ecosystem level, well beyond the level of the events of eating and being eaten.\nThese seasonal population updates occur in the time scale of months/years and in general we can write a model as follows. $$\\dot{N_i}(t)=\\lambda \\sum_j g_{ij}(t)N_i(t) - \\sum_j g_{ji}(t)N_j(t)-d_iN_i(t) \\quad (1)$$\nThe parameters \\( \\lambda\\) and \\(d_i\\) correspond to the efficiency at which the resources transfer from a prey to its predator, and the natural death rates.\nTo complete such a model we need to specify the predation rates \\(g_{kl}(t) \\) between species \\( k \\) and \\( l \\). However, the goal here is more ambitious, we want to know, how does an ecosystem is formed in the first place! As well as how does natural selection drives the system to robust and stable configurations, in which the system \\( (1) \\) operates.\nTo do so, we might, first consider a starting ecosystem consisting of a single species and establish the criteria for species evolution, and how to determine if two species posses a predator/prey relationship.\nSpecies features and mutations. # Species are defined by a set of phenotypical features. This is a set of \\(L \\) different integers \\( f_i=\\{n_j\\}_{j=1}^L \\) taken from a pool of \\( K \\) possible values. Each of the features is compared against each other by a score encoded in a anti-symmetric \\( K\\times K \\) matrix \\(\\bf{M}\\) which contains a random number. Then if we consider two species \\( i,j \\). We score them by computing: $$S_{ij} = \\max \\lbrace 0,\\frac{1}{L}\\sum_{a \\in i, \\thinspace b \\in j} M_{a,b}\\rbrace \\quad(2)$$\nWhich allow us to determine if there exists a predator prey relationship between \\( i \\) and \\( j\\). With this definition of species and species score \\( (2) \\) we can treat the external resources as species \\( \u0026ldquo;0\u0026rdquo; \\), with a string fixed and a large population value \\( R \\).\nChoice of diet, rates and evolutionarily stable strategies. # Once that we have a definition of species and a way to compare them. We can assume that species which are close in terms of features and posses a common prey compete harder for a common prey between them, in comparison with species which are not very similar. In fact, inter-specific competition will be the hardest. To achieve this, the model defines the competition score between species \\(i\\) and \\(j\\) as: $$\\alpha_{ij} = c+(1-c)q_{ij}\\quad(3)$$\nWhere \\(c \\in (0,1)\\) is a competition parameter and \\(q_{ij}\\) is the fraction of common features between species.\nFor the rates \\( g_{ij}(t) \\) the model uses the generalised ratio dependent functional response:\n$$g_{ij} = \\frac{ f_{ij}S_{ij}N_j }{bNj + \\sum_{k} \\alpha_{ki}f_{kj}S_kjN_k} \\quad(4)$$\nThis choice \\( (4) \\) introduces the fractions of effort \\( f_{ij} \\) a species \\( i \\) puts into foraging for \\(j \\). In other words:\n$$ f_{ij} = \\frac{g_{ij}}{\\sum_j g_{ij}} \\quad(5)$$\nEquations \\( (4) \\) and \\( (5) \\) are re-adjusted each time the populations change. These, determine the foraging strategy of the each predator. This choice of efforts is an evolutionary stable strategy (ESS), which means that any other strategy can not be successful against it, provided the ESS is already taken by the majority. The dynamics of the efforts takes place in the shorter time scale of the model, which is that of hours/days.\nEvolution of the system in between evolutionary steps, showing the population updates and the adjustment of the efforts (links thickness). In this particular set, we can also observe an extinction event, which is determined once a population reaches a value below 1. For a fixed set of species, the foraging and population dynamics will eventually lead to a stationary state, where nothing will change any further. Once such a fixed point is reached, a new species is added to the system. New species are introduced, by choosing any of the surviving species and replacing one of the features for a new one (evolutionary step). The new species starts with a population of one individual and the short and intermediate time scale dynamics starts again. ","date":"23 January 2025","externalUrl":null,"permalink":"/posts/Python%20in%20the%20browser%20-%20Evolving%20networks/","section":"Posts","summary":"","title":"Python in the browser:  Evolving Networks.","type":"posts"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/feature-extraction/","section":"Tags","summary":"","title":"Feature Extraction","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/image-analysis/","section":"Tags","summary":"","title":"Image Analysis","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/microtubules/","section":"Tags","summary":"","title":"Microtubules","type":"tags"},{"content":" In this post I describe a small, yet fun, analysis that might be of use to researchers in the field of microtubules. The goal is to extract estimates of the amount and length of polymer from images of cortical networks such as the one below, courtesy of the Ram Dixit lab.\nThe polymer quantities estimated here can be used to postulate, inform, verify or constrain theoretical, mathematical and computational models of cytokeletal dynamics. Such models can become quite complicated, so, with these estimations are aimed to keep, calibrate or test the models to be within feasible biological, physiological and mechanical limits.\nTime evolution of cortical microtubule networks in plant hypocotyl cells. This system is an example of a biological complex system out of equilibrium.\nThe networks shown are microscopy images in which the flourescent signal is labeling the tubulin which composes the polymer filaments known as microtubules. The microtubules polymerise and de-polymerise at the tip, and are anchored to the cell membrane, in the inner surface of the cell wall.\nThe interaction between microtubules is mechanical due to collision events. An outcome of a collision is that the tip of the incoming tubule can be lost, which results in a rapid depolymerisation or catastrophe of the filament. Other possible scenarios are: the incoming filament simply crosses over the target filament, or the two filaments zip together forming a bundle.\nThe frequency of such outcomes, the geometric constraints and some other chemical and mechanical processes determine the organisation of the network. Which can be aligned arrays, like the ones in the movie, or disordered arrays.\nThis very lousy introduction to the topic is all I am going to mention, but I really encourage you to consult a proper reference in the subject, such as this.\nExtracting the length and amount of polymer using python. # Pre-requisites and libraries. # The main library used is numpy\nShapely - Very handy geometry package.\nScikit-image - A very well developed and supported image processing library.\nMatplotlib and pandas for plotting and data processing are also used.\nStep 1 - Cell of interest segmentation. # We will only be focusing on the cell at the center of the movie. So, the first step is to segment the region of interest across all the frames. This can be easily done by selecting a small set of points outlining the shape of the cell. This can be done using a program such as FIJI/ImageJ or the point picker GUI wrote ages ago to annotate microtubule collisions.\nTo form a polygon, the first and last points need to be the same. Once you have the set outlining the cell shape, and stored in a file, we need to load these in any way you prefer. Next, we load the tiff file. For this purpose I use scikit-image.\nimport skimage.io as io import matplotlib.pyplot as plt import numpy as np import pandas as pd from shapely.geometry import Polygon, Point All the modules required for this post to work. Assuming that the image stack file is named cortex.tiff, then it can be loaded as follows:\nfile = \u0026#39;cortex.tiff\u0026#39;; Z = io.imread(file); And that's that! The stack is loaded into a numpy array. By using `Z.shape`, we can obtain information about the movie. In my case `print(Z.shape)` returns the tuple `(200, 512, 512)` which tells us that there are 200 frames, each frame containing a 512x512 array image. To mask each frame, we need to set everything outside the region of interest to zero and keep the inside pixels unaltered.\nStep I. Region of interest segmentation. Straightforward, yet essential. The figure above shows a masked frame. We need to mask all the frames, to do so, first we generate a polygon object and store all the inner points in a list:\ncoords=[(i,j) for i,j in zip(list(X),list(Y))] pl = Polygon(coords) minx, miny, maxx, maxy = pl.bounds minx, miny, maxx, maxy = int(minx), int(miny), int(maxx), int(maxy) box_patch = [[x,y] for x in range(minx,maxx) for y in range(miny,maxy)] pixels = [] for pb in box_patch: pt = Point(pb[0],pb[1]) if(pl.contains(pt)): pixels.append([int(pb[0]), int(pb[1])]) The above snippet computes all the pixels inside the shapely polygon object pl. This is assuming that the polygon point coordinates are stored in the arrays X and Y. Then, to get the masked array, we apply something like the snippet below to every frame:\nA = Z[n,:,:] B = np.zeros(B.shape) for pix in pixels: B[pix[1],pix[0]]=A[pix[1],pix[0]] By calling pl.area, we get the value of the area enclosed by the polygon, which we will use later to compute the desired length and amount polymer estimates. So far, we have been using pixel units. To translate any results to actual length units, we need the scale conversion factor from the file metadata, In the present example we have \\( \\lambda = 512 px / 40.96 \\mu m = 12.5 \\mu m ^{-1} \\). The cell area \\( A = 93731 px^2 = 600 \\mu m ^ {2}\\).\nAn extra step that can be performed is to scale all the masked frames pixel values to values between zero and one. I did this by applying the following function to the stack Z, using the list of pixels inside the polygon.\ndef scaleZ(Z,px): ZS = np.zeros(Z.shape); mins = []; maxs = []; for k in range(Z.shape[0]): for pix in px: ZS[k,pix[1],pix[0]] = Z[k,pix[1],pix[0]] MAX = np.max(ZS) ZS = ZS/MAX; print(ZS.shape) return ZS Step 2 - Background estimation and removal. # We have a stack of images showing the fluorescence signal \\( F \\) of tubulin. This means the more tubulin, the more intense the signal. If we assume that the variable of interest \\( L \\) is linearly related to the flourescence, then: \\(\\ L(r) = \\alpha F(r) + \\beta \\) for every pixel \\(r\\) in the frame. The task is then to estimate the values of \\( \\alpha \\) and \\( \\beta \\).\nFor the case of \\( \\beta \\), if we interpret the masked image as a surface \\( z = f(r)\\), then the level set \\( z = c_b \\) with the largest number elements, is precisely the background. This can be found by computing the number of pixels on each level in the range of the signal as follows:\nAVDATX = []; AVDATY = []; for q in range(Z.shape[0]): BA = Z[q,:,:] BB = BA[BA \u0026gt; 0] nx = np.unique(BB) ny = [] #Computes the level sets for nj in nx: dyj = BB[BB == nj]; ny.append(len(dyj)) # Computes a smoothed # curve from the data wnx = [] wny = [] dn = 250 for j in range(0,len(nx)-dn): wnx.append(np.mean(nx[j:j+dn])) wny.append(np.mean(ny[j:j+dn])) AVDATX.append(wnx); AVDATY.append(wny); THX=[] # Loops over the smooth versions # and finds the pair (xmax,ymax) which # maximises the curve for j in range(ZS.shape[0]): ymax = max(AVDATY[j]) xj = AVDATY[j].index(ymax) max = AVDATX[j][xj] THX.append(xmax); The video below illustrates the results of what I just described for a single frame.\nPyvista rendering of a frame as surface. (Left) full frame, (Middle) the masked frame. (Right) The signal with the background substracted. Applying the snippet above to every masked frame and collecting every xmax and ymax values, and plotting the results, we get something that looks like:\n(Left) Counts of the number of elements on each level set of the masked frames, for every frame. The black plots correspond to the respective smoothed signals using a sliding window, the black lines correspond to the maximum value of each smoothed plot. (Right) The smoothed signals, without the raw level set counts, alongside the set which the maximal numberf of elements (the background). From the plots above it es easy to set the background value as the average of values in which the plots are maximised. In this case it give a signal value of around 0.26.\nResults of removing the background in every frame (Right) versus the original data (Left). We now need carry out the calibration step to obtain the value of \\( \\alpha \\).\nStep 3 - Tubulin/Intensity ratio estimation. # This is the final step. In a nutshell, it consists of picking a segment of what appears to be a single microtubule segment. If the segment has a length \\( l_o \\) and we measure the amount of signal \\( I_o \\) within a box of that length and height \\( h \\). Then we just find the value \\( \\alpha \\) such as \\( \\ l_o = \\alpha I_o \\).\nIn other words \\( \\alpha = \\frac{l_o}{I_o} \\). Therefore, if a given amount of signal \\( I \\) is measured, we can say that it can be accommodated in a length \\( L = \\frac{I}{I_o} l_o\\). By doing so, we can then calculate the full amount of signal in every frame and thus make an estimation of how long does a filament need to be to accommodate that amount of signal.\nThe first thing to do is to navigate through the movie and select a few segments which then will be used to make the calibration. This step is again a step that can be carried out using an external application to save points.\nSelecting a single filament by marking it allows to work on a small box around the microtubule signal. the sequence above sumarises the steps, until we have a box containing the filament with the background removed. If P1 =(X1,Y1) and P2 = (X2,Y2) are the coordinates, defining the segment. I chose to rotate the image around Pa to align the segment with the X axis. This is easily done using the rotate method from scikit image.\ndef Box(Zm, X, Y, dn, bkg): X1=X[0]; Y1=Y[0]; X2=X[1];Y2=Y[1]; if Y1 \u0026lt; Y2: Xa = X1; Xb = X2 Ya = Y1; Yb = Y2 else: Xa = X2; Xb = X1; Yb = Y1; Ya = Y2; Ux=Xb-Xa; Uy=Yb-Ya; R=np.sqrt(Ux**2+Uy**2) angle=np.arccos(Ux/R); angdeg=angle*(180/np.pi); Q = rotate(Zm,angdeg,center=(Xa,Ya), resize=False,preserve_range=True); n1=int(Xa); n2=int(Xa+R) m1=int(Ya)-dn; m2=int(Ya)+dn P = Q[m1:m2,n1:n2] Q = P - bkg; Q[Q\u0026lt;=0] = 0.0 return Q Function to compute box with the marked filament. The arguments are the frame Zm, the filament end points X,Y, the box height dn and the background value bkg. Once we have the box determined, we can systematically vary the width of from zero to values which contains all the signal inside. Then, the criteria to used the amount of signal of a single filament is: the box with width for which the amount of signal start to be almost constant.\nn1 = no n2 = no+1 P = Q[n1:n2,0:Na] dn.append(n2-n1) In.append(sum(P.ravel())) for n in range(no-1): n1 -=1 n2 += 1 P = Q[n1:n2,:] dn.append(n2-n1) In.append(sum(P.ravel())) Snippet to compute the signal within a box of height 2no. The arguments are no and Na which is the length of the box, as well as the array Q. Output from performing the calibration step over a segment. (Left) Measures of the amount of signal in the box. (Right) The filament enclosed in boxes of several heights. By choosing several filaments across the movie, we can obtain several points and average to estimate the ratio signal/length.\n#Average of the single segments from the previous step Sm = avth/(Q.shape[0]*Q.shape[1]) #Ratio pixels/length in microns rj=512/40.96 #Box length in microns dx = Lmin/rj #Lists to store the Amount of polymer and length Lk = [];Ik=[]; Area = pl.area #Area in microns^2 Amu = Area/(rj*rj) #Length of equivalent area square DLA = np.sqrt(Amu) for j in range(ZS.shape[0]): Zm = ZS[j,:,:] - bg Zm[Zm\u0026lt;=0] = 0.0 Q = Zm.ravel() Qa = Q[Q\u0026gt;=Sm] Qn = sum(Zm.ravel()) Lk.append((Qn/avth)*dx) Ik.append(Qn/Amu) nmin = np.min(Qa); nmax = 1 bins = np.arange(nmin,nmax,0.005) yh, bn = np.histogram(Qa, bins=bins) Lkav = np.mean(Lk) SLkav = np.std(Lk) Ikav = np.mean(Ik) SIkav = np.std(Ik) From the previous step we know that \\( L = l_o \\left( \\frac{I}{I_o} \\right) \\). Whereas the amount of polymer is the sum of the intensity above or equal to the calibration amount \\( I_o. \\)\nExecuting the snippet and collecting the mean values we get an average linear polymer length per area of \\( 1.52 \\pm 0.13 \\mu m \\) and the average amount of polymer per cell area of \\( 1.73 \\pm 0.15 [\\mu m ]^{-2} \\)\nI have prepared a jupyer notebook available here, to be used as you please.\nThe full analysis will probably find its way into an article appendix with some luck. If it is not the case, well, at least for now, here it is. To be read and maybe be used by someone interested!\nUseful tools. # Microtubule Picker - Used in step 1 ImageJ/FIJI - Used in step 1\nShapely - Geometry package\nScikit-image - General image processing library\n","date":"27 December 2024","externalUrl":null,"permalink":"/posts/microtubules---cortical-networks-feature-extraction./","section":"Posts","summary":"","title":"Microtubules! - Cortical networks feature extraction.","type":"posts"},{"content":" Studying molecular evolution presents a lot of opportunities to venture into interesting side quests, such as the content of this post. Here, I showcase ways to visualise complex networks with some degree of interactivity.\nThese little renderings were conceived for lectures, talks and videos and rarely make their way into an academic publication, despite their didactic value. The reason is very simple, publications are a two dimensional, almost static version of media. However, technologies such as Jupyter, WebGL and other rendering backend libraries have been available for a long time now, and their corresponding api\u0026rsquo;s made very easy to use for everybody.\nFor the sake of brevity, let's cut to the case, here I am going through how and why I made renderings such as the ones below and what exactly these represent. Viewer I. # The genetic code mutation network, which is the example used for this post. It should work on most devices, but for a full session of fun, try it with a mouse on a separate tab. The recommended way to play with the viewer, is on single window, which can be accessed here and a legacy version with draggable nodes here\nWhat is this network? : The genetic code. # For theoreticians, molecular evolution is often a problem to become familiar with stochastic processes, as mutation and recombination of sequences which generate new genetic variants in DNA and RNA are probabilistic. On its simplest form at least, the theory is also the prefect segway to understand the origin of important features present in living systems, such as the concepts of robustness of phenotypes to perturbations of the genotype as well as evolvability or the origin of the great variability observed in the phenotypes.\nThese evolutionary features are also somewhat universal, and these are observed in systems of very different scale and nature, such as molecules, gene-regulation circuits and networks and metabolic networks.\nBoth, robustness and evolvability are linked to the topology of the neutral genotype networks, which are defined by all the possible genotypes as nodes and linked if these are a mutation away.\nTake for instance the following table containing the genetic code:\nGenetic code Phe UUU Ser UCU Tyr UAU Cys UGU Phe UUC Ser UCC Tyr UAC Cys UGC Leu UUA Ser UCA Stp UAA Stp UGA Leu UUG Ser UCG Stp UAG Trp UGG Leu CUU Pro CCU His CAU Arg CGU Leu CUC Pro CCC His CAC Arg CGC Leu CUA Pro CCA Gln CAA Arg CGA Leu CUG Pro CCG Gln CAG Arg CGG Ile AUU Thr ACU Asn AAU Ser AGU Ile AUC Thr ACC Asn AAC Ser AGC Ile AUA Thr ACA Lys AAA Arg AGA Met AUG Thr ACG Lys AAG Arg AGG Val GUU Ala GCU Asp GAU Gly GGU Val GUC Ala GCC Asp GAC Gly GGC Val GUA Ala GCA Glu GAA Gly GGA Val GUG Ala GCG Glu GAG Gly GGG Each cell contains, on the left, the aminoacid (phenotype) associated with the codon sequence (genotype) on the right. In general for an alphabet of four characters like \\( \\{U,G,A,C\\} \\) there are \\( 4^L \\) unique sequences of length \\( L \\). Each one with \\( 3 L\\) neighbour sequences one character away from it. Using this and grouping the codons by aminoacid we get the following plot.\nThe genetic code network. Click me. Although (to my taste) the graph is very pretty, it is very difficult to get any any useful information easily. However if we remove all the links from one phenotype to another, keeping just the mutation links within the same aminoacid (synonymous mutations) it is immediate to see what it is meant by robustness, as all cases with more than a single codon, posses mutations which leave the phenotype unchanged. The genetic code network II - Synonymous components. Click me. Now, the most important bit. Evolvability and how is it reflected in these diagrams. By choosing an aminoacid and plotting only the links from its nodes to the rest of the system, we can see how easy is to navigate the network from one aminoacid to all the other ones by only a few mutations, specially for those phenotypes with the largest number of codons.\nAlanine, Arginine and Asparagine networks. Click on them! Aspartic acid, Cysteine and Glutamine networks. Click on them! Glutamine acid, Glycine and Histidine networks. Click on them! Isoleucine, Leucine and Lysine networks. Click on them! Methionine, Penylalanine and Proline networks. Click on them! Serine, Stop and Threonine networks. Click on them! Tryptophan, Tyrosin and Valine networks. Click on them! As we can see from these awesome plots (to my taste) even this simple system contains enough complexity to make the graphical representation of the main ideas quite convoluted. For comparison purposes and fun, all the information so far presented is shown in the same form in the viewer already presented at the beginning of this entry, and also, in the same format as the plots above, in the following renderer.\nViewer II. # The genetic code mutation network and its components. This should work on most devices, but for a full session of fun, try it with a mouse on a separate tab. (Link here!) Final comments. # The properties of resultant graph or phenotype landscape allow us to treat the problem of evolution as problem of navigating in a complex network. This is a problem suitable to be treated with tools of statistical physics. If we posses the knowledge of a well defined genotype-phenotype mapping. This is of course a condition not easily satisfied in the vast majority of real systems. However, the theoretical framework is fairly universal. As an example of this, the plot below shows the case of of the connections between two secondary structures for the case of RNA sequences of length 12. For relatively short lengths, the secondary structure can be considered as a good approximation or proxy for the genotype-phenotype mapping. Detail of the connections between two RNA secondary structures of length 12. Showing all the sequences which fold into such structures and connected to other sequences if these are one mutation away. Again, we can see that it is possible to mutate inside and outside the secondary structures, the folding temperature for this structures is T=27 C. Finally, these are links for the repository containing the code, as well as for the viewers.\nGithub repository.\nViewer I.\nViewer II.\nLegacy viewer with draggable nodes.\n","date":"11 December 2024","externalUrl":null,"permalink":"/posts/the-genetic-code-network/","section":"Posts","summary":"","title":"Interactive network visualisation - The Genetic Code","type":"posts"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/molecular-evolution/","section":"Tags","summary":"","title":"Molecular Evolution","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/networks/","section":"Tags","summary":"","title":"Networks","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/threejs/","section":"Tags","summary":"","title":"Three,js","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/two.js/","section":"Tags","summary":"","title":"Two.js","type":"tags"},{"content":" Hello and welcome! My name is Carlos, I am a theoretical physicist interested in the application of physics, mathematics, data and computers to the study and solution of problems arising in the area of complex systems. I create mathematical and physical models to describe how systems composed by many interacting elements evolve in time and space. I also numerically solve models to better understand how a system works, e.g. making predictions and forecasting all the possible scenarios which a system can undergo, or predicting a system's response to perturbations, treatments and interventions. I have applied my expertise in many areas, including research into the formation and evolution of ecosystems, molecular evolution, the origin of cardiac arrhythmias in the human heart, pattern development in petals and chemo-mechanics of the plant cell cortex.\nThe tools I employ can be broadly classified in two categories, Numerical and Theoretical.\nThe theoretical approaches include applications of the theory of dynamical systems and non-linear dynamics, chemical kinetics, game theory, mechanics and elasticity , population genetics and population dynamics, the theory of excitable systems and complex networks as useful conceptual frameworks. I often use statistical learning and inference to quantify the results.\nThe numerical aspect involves the solution of the equations for the simulations of the systems under study, which might consist of the development of optimization schemes used in the finite element method for solving PDEs, explicit and implicit schemes to solve ODEs, or Monte-Carlo methods for stochastic simulations. All these techniques require development and deployment in a computer.\nI also have a very strong interest in data visualization, data analysis, interactive simulations, and automation of physical computing devices and sensors for data collection, outreach demonstrations and experiments. Basically, every now and then I build robots and machines to do something fun, and at a very very very low cost .\nMany of the projects I have worked on are described here, the codes and detailed instructions are archived and documented in my github , gitlab and Hackster pages.\n","date":"10 December 2024","externalUrl":null,"permalink":"/about/bio/","section":"Abouts","summary":"","title":"About me","type":"about"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/about-me/","section":"Tags","summary":"","title":"About Me","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/about/","section":"Abouts","summary":"","title":"Abouts","type":"about"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/barcelona/","section":"Tags","summary":"","title":"Barcelona","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/cambridge/","section":"Tags","summary":"","title":"Cambridge","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/experience/","section":"Experiences","summary":"","title":"Experiences","type":"experience"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/madrid/","section":"Tags","summary":"","title":"Madrid","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/mathematical-modelling/","section":"Tags","summary":"","title":"Mathematical Modelling","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/numerical-solutions/","section":"Tags","summary":"","title":"Numerical Solutions","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/optimisation/","section":"Tags","summary":"","title":"Optimisation","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/physics/","section":"Tags","summary":"","title":"Physics","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming","type":"tags"},{"content":" Hello Everyone! Welcome to my humble corner of the digital world. My name is Carlos, I am a Cambridge U.K. based physicist, and in this site I post content related to my scientific interests, as well as some fun things I would like to share. Below is a brief overview of my career so far. You can also check out my ORCID and Linkedin profiles.\nI got my Bachelor\u0026rsquo;s degree from UNAM in Mexico City, working on stochastic models of city-size distributions in space and time.\nI got my PhD from the University of Manchester in the U.K. under the supervision of Prof. Alan J. McKane. I worked on evolution of food-web structures and stochastic population dynamics in space and time.\nUniversitat Politècnica de Catalunya (Barcelona, Spain) Applied Physics Department, working with Prof. Blas Echeberria. During this post, I worked on detailed mathematical models and numerical simulations of calcium cycling in human atrial cells and the origin of cardiac arrhythmias.\nCentro de Astrobiología, CSIC-INTA (Madrid, Spain). As part Prof. Susanna Manrubia’s research group I carried out research related to the topology induced in the space of phenotypes by genotype-phenotype mappings for the case of RNA molecules.\nUniversity of Cambridge - Department of Plant Sciences (Cambridge U.K). As part of Prof. Beverley Glover\u0026rsquo;s Evolution and Development group, I worked on the mechanical origins of petal epidermal cells patterns.\nUniversity of Cambridge - Sainsbury Laboratory (Cambridge U.K.). Here working with Francois Nèdelec, I carried out investigations of the dynamical origins of order-disorder transitions in the organization of the filaments composing the plant cortex.\nIn all these posts, I worked on finding the mechanisms underlying a certain phenomena. I applied the scientific method to postulate a set of hypotheses, abstract these hypotheses into mathematical models, then solved the models and studied the solutions to make predictions.\nI have also worked in Bioinformatics at TSL in Norwich U.K. and as a scientific programmer at The European Bioinformatics Institute (EMBL-EBI) in Cambridge. In my spare time I work on robots, data visualisation, computer vision, AI and automation.\n","date":"10 December 2024","externalUrl":null,"permalink":"/experience/resume/","section":"Experiences","summary":"","title":"Resume","type":"experience"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/work-experience/","section":"Tags","summary":"","title":"Work-Experience","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]