[{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/","section":"Carlos's Website","summary":"","title":"Carlos's Website","type":"page"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/data-visualization/","section":"Tags","summary":"","title":"Data Visualization","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/feature-extraction/","section":"Tags","summary":"","title":"Feature Extraction","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/image-analysis/","section":"Tags","summary":"","title":"Image Analysis","type":"tags"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/microtubules/","section":"Tags","summary":"","title":"Microtubules","type":"tags"},{"content":" In this post I describe a small, yet fun, analysis which might be of use to researchers in the field of microtubules. The goal is to extract estimates of the amount of polymer and linear densities of cortical network data from cells evolving in time such as the one below, courtesy of the Ram Dixit lab.\nThe quantities estimated here are can be used to postulate, inform, verify or constraint theoretical or computational models of cytokeletal dynamics. Such model can become quite complicated, so, with these estimations we can keep the simulations within feasible biological, physiological and mechanical limits.\nTime evolution of a cortical microtubule networks in plant hypocotyl cells. This system is an example of a biological complex system out of equilibrium.\nThe networks shown are flourescent microscopy images which label tubulin. As it can be seen, the filaments (microtubules) polymerise and de-polymerise at the tip. These are anchored to the cell membrane, in the inner surface of the cell wall.\nThe interaction between microtubules is mechanical due to collision events. An outcome of a collision is that the tip of the incoming tubule can be lost, which results in a rapid depolymerisation or catastrophe of the filament. Other possible scenario are that the incoming filament simply crosses over the target filament, or that the two filaments zip together forming a bundle.\nThe frequency of such outcomes, as well as geometric contraints and some other chemical and mechanical processes determine the organisation of the network. Which can be aligned arrays, like the ones in the movie, or disordered arrays.\nFor the sake of brevity, this very lousy introduction to the topic is all I am going to mention, but I really encourage you to consult a proper reference in the subject, such as this.\nExtracting the length and amount of polymer using python. # Pre-requisites. # The main library used is numpy\nShapely - Very handy geometry package.\nScikit-image - A very well developed and supported image processing library.\nI use Pyvista for the 3d renderings but as you will see, it is entirely optional.\nMatplotlib and pandas for plots and data processing are also used, but by no means essential.\nStep 1 - Cell of interest segmentation. # We will only use the cell at the center of the movie. So the first step is to segment the region of interest across all the frames. This can be easily done by selecting a small set of points outlining the shape of the cell. This can be done using a program such as FIJI/ImageJ or the point picker GUI wrote ages ago to annotate microtubule collisions.\nTo form a polygon, the first and last points need to be the same. Once you have the set outlining the cell shape, and stored in a file, we need to load these in any way you prefer. Next, we load the tiff file. For this purpose I use scikit-image.\nimport skimage.io as io import matplotlib.pyplot as plt import numpy as np import pyvista as pv import pandas as pd import glob as glob from shapely.geometry import Polygon, Point All the modules required for this post to work. If the movie stack file is named cortex.tiff then to import it we invoke:\nfile = \u0026#39;cortex.tiff\u0026#39;; Z = io.imread(file); And that's that! the stack is loaded into a numpy array. By using `Z.shape`, we can obtain information about the movie. In my case `print(Z.shape)` returns the tuple `(200, 512, 512)` which tells us that there are 200 frames, each frame containing a 512x512 array image. Once the points and the stack are loaded we can mask each frame by setting everything outside the region of interest to zero and keep the inside pixels unaltered.\nStep I. Region of interest segmentation. Straightforward, yet essential. The figure above shows a masked frame, this needs to be applied to every frame. This can be done as follows:\nFirst, generate a polygon object and store all the inner points in a list.\ncoords=[(i,j) for i,j in zip(list(X),list(Y))] pl = Polygon(coords) minx, miny, maxx, maxy = pl.bounds minx, miny, maxx, maxy = int(minx), int(miny), int(maxx), int(maxy) box_patch = [[x,y] for x in range(minx,maxx) for y in range(miny,maxy)] pixels = [] for pb in box_patch: pt = Point(pb[0],pb[1]) if(pl.contains(pt)): pixels.append([int(pb[0]), int(pb[1])]) The above snippet computes all the pixels inside the shapely polygon object. This is of course assuming the polygon point coordinates are stored in the arrays X and Y. Then, to get the masked array, we apply something like the snippet below to every frame:\nA = Z[n,:,:] B = np.zeros(B.shape) for pix in pixels: B[pix[1],pix[0]]=A[pix[1],pix[0]] By calling pl.area, we get the value of the area enclosed by the polygon, which we will use later to compute the desired length and amount polymer estimates. So far, we have been using pixel units. To translate any results to actual length units, we need the scale conversion factor from the file metadata, In the present example we have \\( \\lambda = 512 px / 40.96 \\mu m = 12.5 \\mu m ^{-1} \\). The cell area \\( A = 93731 px^2 = 600 \\mu m ^ {2}\\).\nAn extra step that can be performed is to scale all the masked frames pixel values to take values between zero and one. I did this by applying the following function to the stack Z, using the list of pixels inside the polygon.\ndef scaleZ(Z,px): ZS = np.zeros(Z.shape); mins = []; maxs = []; for k in range(Z.shape[0]): for pix in px: ZS[k,pix[1],pix[0]] = Z[k,pix[1],pix[0]] MAX = np.max(ZS) ZS = ZS/MAX; print(ZS.shape) return ZS Step 2 - Background estimation and removal. # We have a stack of images showing the fluorescence signal \\( F \\) of tubulin. This means the more tubulin, the more intense the signal. If we assume that the variable of interest \\( L \\) is related to the flourescence by: \\(\\ L(r) = \\alpha F(r) + \\beta \\) for every pixel \\(r\\) in the frame, we need to estimate the values of \\( \\alpha \\) and \\( \\beta \\).\nFor the case of \\( \\beta \\), if we interpret the masked image as a surface \\( z = f(r)\\), then the level set \\( z = c_b \\) with the largest number elements, is precisely the background. This can be found by computing the number of pixels on each level in the range of the signal as follows:\nDATY = []; DATX = []; AVDATX = []; AVDATY = [] for q in range(Z.shape[0]): BA = Z[q,:,:] BB = BA[BA \u0026gt; 0] nx = np.unique(BB) ny = [] for nj in nx: dyj = BB[BB == nj]; ny.append(len(dyj)) wnx = [] wny = [] dn = 250 for j in range(0,len(nx)-dn): wnx.append(np.mean(nx[j:j+dn])) wny.append(np.mean(ny[j:j+dn])) AVDATX.append(wnx); AVDATY.append(wny); THX=[] for j in range(ZS.shape[0]): ymax = max(AVDATY[j]) xj = AVDATY[j].index(ymax) max = AVDATX[j][xj] THX.append(xmax); The video below illustrates what I just described for a single frame.\nPyvista rendering of a frame as surface. (Left) full frame, (Middle) the masked frame. (Right) The signal with the background substracted. Applying the snippet above to every masked frame and collecting every nx and ny values, and plotting the results, we get something that looks like:\n(Left) Counts of the number of elements on each level set of the masked frames, for every frame. The black plots correspond to the respective smoothed signals using a sliding window, the black lines correspond to the maximum value of each smoothed plot. (Right) The smoothed signals, without the raw level set counts, alongside the set which the maximal numberf of elements (the background). From the plots above it es easy to set the background value as the average of values in which the plots are maximised. In this case it give a signal value of around 0.26.\nWe now need to calibrate the results by obtaining the value of \\( \\alpha \\).\nResults of removing the background in every frame (Right) versus the original data (Left). Step 3 - Tubulin/Intensity ratio estimation. # This is the final step. It is a calibration step. In a nutshell, it consists of picking a segment of what appears to be a single microtubule segment, and measure the amount of signal per length. By doing so, we can then calculate the full amount of signal in every frame and thus make an estimation of how long does a filament need to be to accommodate that amount of signal. The first thing to do is to navigate through the movie and select a few segments which then will be used to calibrate. This step is again a step that can be carried out using an external application to save points.\nSelecting a single filament by marking it allows to work on a small box around the microtubule signal. the sequence above sumarises the steps, until we have a box containing the filament with the background removed. If P1 =(X1,Y1) and P2 = (X2,Y2) are the coordinates, defining the segment. I chose to rotate the image around Pa to align the segment with the X axis. This is easily done using the rotate method from scikit image.\ndef Box(Zm, X, Y, dn, bkg): X1=X[0]; Y1=Y[0]; X2=X[1];Y2=Y[1]; if Y1 \u0026lt; Y2: Xa = X1; Xb = X2 Ya = Y1; Yb = Y2 else: Xa = X2; Xb = X1; Yb = Y1; Ya = Y2; Ux=Xb-Xa; Uy=Yb-Ya; R=np.sqrt(Ux**2+Uy**2) angle=np.arccos(Ux/R); angdeg=angle*(180/np.pi); Q = rotate(Zm,angdeg,center=(Xa,Ya), resize=False,preserve_range=True); n1=int(Xa); n2=int(Xa+R) m1=int(Ya)-dn; m2=int(Ya)+dn P = Q[m1:m2,n1:n2] Q = P - bkg; Q[Q\u0026lt;=0] = 0.0 return Q Function to compute box with the marked filament. The arguments are the frame Zm, the filament end points X,Y, the box height dn and the background value bkg. Now, that we have the box, we can systematically increase the width of the box from zero to values which contains all the signal inside. The criteria to used to select the width is the signal value in which the width starts to be almost constant.\nn1 = no n2 = no+1 P = Q[n1:n2,0:Na] dn.append(n2-n1) In.append(sum(P.ravel())) for n in range(no-1): n1 -=1 n2 += 1 P = Q[n1:n2,:] dn.append(n2-n1) In.append(sum(P.ravel())) Snippet to compute the signal within a box of height 2no. The arguments are no and Na which is the length of the box, as well as the array Q. To determine the amount of signal in the length provided we select the point in which the signal starts to decrease by decreasing the box height. This is shown in the plots below.\nSnippet to compute the signal within a box of height 2no. The arguments are no and Na which is the length of the box, as well as the array Q. By choosing several filaments across the movie, we can obtain several points and average to estimate the ratio signal/length.\n#Average of the single segments from the previous step Sm = avth/(Q.shape[0]*Q.shape[1]) #Ratio pixels/length in microns rj=512/40.96 #Box length in microns dx = Lmin/rj #Lists to store the Amount of polymer and length Lk = [];Ik=[]; Area = pl.area #Area in microns^2 Amu = Area/(rj*rj) #Length of equivalent area square DLA = np.sqrt(Amu) for j in range(ZS.shape[0]): Zm = ZS[j,:,:] - bg Zm[Zm\u0026lt;=0] = 0.0 Q = Zm.ravel() Qa = Q[Q\u0026gt;=Sm] Qn = sum(Zm.ravel()) Lk.append((Qn/avth)*dx) Ik.append(Qn/Amu) nmin = np.min(Qa); nmax = 1 bins = np.arange(nmin,nmax,0.005) yh, bn = np.histogram(Qa, bins=bins) Lkav = np.mean(Lk) SLkav = np.std(Lk) Ikav = np.mean(Ik) SIkav = np.std(Ik) From the previous step we know that \\( L = l_o \\left( \\frac{I}{I_o} \\right) \\). Whereas the amount of polymer is the sum of the intensity above or equal to the calibration amount \\( I_o. \\)\nExecuting the snippet and collecting the mean values we get an average linear polymer length of \\( 2293 \\pm 217 \\mu m \\) and the average amount of polymer per cell area \\( P_n = 7 \\pm 0.7 [\\mu m ]^{-2} \\)\nAnd that\u0026rsquo;s it! The full script can be found here!.\nThe full analyisis will probably find its way into an article, but if it is not the case, well here its to be read and maybe be used by someone interested!\nUseful tools. # Microtubule Picker - Used in step 1 ImageJ/FIJI - Used in step 1\nShapely - Geometry package\nScikit-image - General image processing library\n","date":"27 December 2024","externalUrl":null,"permalink":"/posts/microtubules---cortical-networks-feature-extraction./","section":"Posts","summary":"","title":"Microtubules! - Cortical networks feature extraction.","type":"posts"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"27 December 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Studying molecular evolution presents a lot of opportunities to venture into interesting side quests, such as the content of this post. Here, I showcase ways to visualise complex networks with some degree of interactivity.\nThese little renderings were conceived for lectures, talks and videos and rarely make their way into an academic publication, despite their didactic value. The reason is very simple, publications are a two dimensional, almost static version of media. However, technologies such as Jupyter, WebGL and other rendering backend libraries have been available for a long time now, and their corresponding api\u0026rsquo;s made very easy to use for everybody.\nFor the sake of brevity, let's cut to the case, here I am going through how and why I made renderings such as the ones below and what exactly these represent. Viewer I. # The genetic code mutation network, which is the example used for this post. It should work on most devices, but for a full session of fun, try it with a mouse on a separate tab. The recommended way to play with the viewer, is on single window, which can be accessed here and a legacy version with draggable nodes here\nWhat is this network? : The genetic code. # For theoreticians, molecular evolution is often a problem to become familiar with stochastic processes, as mutation and recombination of sequences which generate new genetic variants in DNA and RNA are probabilistic. On its simplest form at least, the theory is also the prefect segway to understand the origin of important features present in living systems, such as the concepts of robustness of phenotypes to perturbations of the genotype as well as evolvability or the origin of the great variability observed in the phenotypes.\nThese evolutionary features are also somewhat universal, and these are observed in systems of very different scale and nature, such as molecules, gene-regulation circuits and networks and metabolic networks.\nBoth, robustness and evolvability are linked to the topology of the neutral genotype networks, which are defined by all the possible genotypes as nodes and linked if these are a mutation away.\nTake for instance the following table containing the genetic code:\nGenetic code Phe UUU Ser UCU Tyr UAU Cys UGU Phe UUC Ser UCC Tyr UAC Cys UGC Leu UUA Ser UCA Stp UAA Stp UGA Leu UUG Ser UCG Stp UAG Trp UGG Leu CUU Pro CCU His CAU Arg CGU Leu CUC Pro CCC His CAC Arg CGC Leu CUA Pro CCA Gln CAA Arg CGA Leu CUG Pro CCG Gln CAG Arg CGG Ile AUU Thr ACU Asn AAU Ser AGU Ile AUC Thr ACC Asn AAC Ser AGC Ile AUA Thr ACA Lys AAA Arg AGA Met AUG Thr ACG Lys AAG Arg AGG Val GUU Ala GCU Asp GAU Gly GGU Val GUC Ala GCC Asp GAC Gly GGC Val GUA Ala GCA Glu GAA Gly GGA Val GUG Ala GCG Glu GAG Gly GGG Each cell contains, on the left, the aminoacid (phenotype) associated with the codon sequence (genotype) on the right. In general for an alphabet of four characters like \\( \\{U,G,A,C\\} \\) there are \\( 4^L \\) unique sequences of length \\( L \\). Each one with \\( 3 L\\) neighbour sequences one character away from it. Using this and grouping the codons by aminoacid we get the following plot.\nThe genetic code network. Click me. Although (to my taste) the graph is very pretty, it is very difficult to get any any useful information easily. However if we remove all the links from one phenotype to another, keeping just the mutation links within the same aminoacid (synonymous mutations) it is immediate to see what it is meant by robustness, as all cases with more than a single codon, posses mutations which leave the phenotype unchanged. The genetic code network II - Synonymous components. Click me. Now, the most important bit. Evolvability and how is it reflected in these diagrams. By choosing an aminoacid and plotting only the links from its nodes to the rest of the system, we can see how easy is to navigate the network from one aminoacid to all the other ones by only a few mutations, specially for those phenotypes with the largest number of codons.\nAlanine, Arginine and Asparagine networks. Click on them! Aspartic acid, Cysteine and Glutamine networks. Click on them! Glutamine acid, Glycine and Histidine networks. Click on them! Isoleucine, Leucine and Lysine networks. Click on them! Methionine, Penylalanine and Proline networks. Click on them! Serine, Stop and Threonine networks. Click on them! Tryptophan, Tyrosin and Valine networks. Click on them! As we can see from these awesome plots (to my taste) even this simple system contains enough complexity to make the graphical representation of the main ideas quite convoluted. For comparison purposes and fun, all the information so far presented is shown in the same form in the viewer already presented at the beginning of this entry, and also, in the same format as the plots above, in the following renderer.\nViewer II. # The genetic code mutation network and its components. This should work on most devices, but for a full session of fun, try it with a mouse on a separate tab. (Link here!) Final comments. # The properties of resultant graph or phenotype landscape allow us to treat the problem of evolution as problem of navigating in a complex network. This is a problem suitable to be treated with tools of statistical physics. If we posses the knowledge of a well defined genotype-phenotype mapping. This is of course a condition not easily satisfied in the vast majority of real systems. However, the theoretical framework is fairly universal. As an example of this, the plot below shows the case of of the connections between two secondary structures for the case of RNA sequences of length 12. For relatively short lengths, the secondary structure can be considered as a good approximation or proxy for the genotype-phenotype mapping. Detail of the connections between two RNA secondary structures of length 12. Showing all the sequences which fold into such structures and connected to other sequences if these are one mutation away. Again, we can see that it is possible to mutate inside and outside the secondary structures, the folding temperature for this structures is T=27 C. Finally, these are links for the repository containing the code, as well as for the viewers.\nGithub repository.\nViewer I.\nViewer II.\nLegacy viewer with draggable nodes.\n","date":"11 December 2024","externalUrl":null,"permalink":"/posts/the-genetic-code-network/","section":"Posts","summary":"","title":"Interactive network visualisation - The Genetic Code","type":"posts"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/molecular-evolution/","section":"Tags","summary":"","title":"Molecular Evolution","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/networks/","section":"Tags","summary":"","title":"Networks","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/threejs/","section":"Tags","summary":"","title":"Three,js","type":"tags"},{"content":"","date":"11 December 2024","externalUrl":null,"permalink":"/tags/two.js/","section":"Tags","summary":"","title":"Two.js","type":"tags"},{"content":" Hello and welcome! My name is Carlos, I am a theoretical physicist interested in the application of physics, mathematics and computers to the study and solution of problems arising in the area of complex systems. By combining data, mathematics, physics we can write down mathematical and physical models to describe how systems composed by many interacting elements evolve in time and space. To understand how a system works, make predictions and forecast all the possible scenarios which a system can undergo, as well as the response to perturbations, treatments and interventions, we often require to numerically solve the models. These are two of my areas of expertise. I have carried out research in the formation and evolution of ecosystems, molecular evolution, the origin of cardiac arrhythmias in the human heart, pattern development in petals and chemo-mechanics of the plant cell cortex, to mention some.\nThe tools employed in such works can be broadly classified in two categories, Numerical and Theoretical.\nThe theoretical approaches include applications of the theory of dynamical systems and non-linear dynamics, chemical kinetics, game theory, mechanics and elasticity , population genetics and population dynamics, the theory of excitable systems and complex networks as useful conceptual frameworks. And statistical learning and inference to quantify the results.\nThe numerical aspect involves the solution of the equations involved in the simulations of the systems under study, which might consist of the development of optimizations schemes used in the finite element method for solving PDEs, explicit and implicit schemes to solve ODEs, Monte-Carlo methods for stochastic simulations. All these techniques, require the development and deployment of such schemes in a computer.\nAlongside the above two categories, I have developed a very strong interest in data visualization, data analysis, interactive simulations, and automation of physical computing devices and sensors for data collection, outreach demonstrations and experiments. Basically, every now and then I build robots and machines to do something fun, and at a very very very low cost .\nMany of the projects I have worked in are described here, the codes and detailed instructions are archived and documented in my github , gitlab and Hackster pages.\n","date":"10 December 2024","externalUrl":null,"permalink":"/about/bio/","section":"Abouts","summary":"","title":"About me","type":"about"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/about-me/","section":"Tags","summary":"","title":"About Me","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/about/","section":"Abouts","summary":"","title":"Abouts","type":"about"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/barcelona/","section":"Tags","summary":"","title":"Barcelona","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/cambridge/","section":"Tags","summary":"","title":"Cambridge","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/experience/","section":"Experiences","summary":"","title":"Experiences","type":"experience"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/madrid/","section":"Tags","summary":"","title":"Madrid","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/mathematical-modelling/","section":"Tags","summary":"","title":"Mathematical Modelling","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/numerical-solutions/","section":"Tags","summary":"","title":"Numerical Solutions","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/optimisation/","section":"Tags","summary":"","title":"Optimisation","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/physics/","section":"Tags","summary":"","title":"Physics","type":"tags"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming","type":"tags"},{"content":" Hello Everyone! and welcome to my humble corner of the digital world. My name is Carlos, I am a Cambridge U.K. based physicist, and in this site I post content related to my scientific interests and some fun things I ended up working on and I would like to share. Below, I list a few of the academic and professional positions I have hold so far. A detailed list can be found in my ORCID and Linkedin profiles.\nI got my degree from UNAM in Mexico City, working on stochastic models of city-size distributions in space and time.\nI got my PhD from the University of Manchester in the U.K. under the supervision of Prof. Alan J. McKane. There, I worked on evolution of food-web structures and stochastic population dynamics in space and time.\nUniversitat Politècnica de Catalunya (Barcelona, Spain) Applied Physics Department, working with Prof. Blas Echeberria. During this post, I worked on detailed mathematical models and numerical simulations of calcium cycling in human atrial cells and the origin of cardiac arrhythmias.\nCentro de Astrobiología, CSIC-INTA (Madrid, Spain). As part Prof. Susanna Manrubia’s research group I carried out research related to the topology induced in the space of phenotypes by genotype-phenotype mappings for the case of RNA molecules.\nUniversity of Cambridge - Department of Plant Sciences (Cambridge U.K). As part of Prof. Beverley Glover\u0026rsquo;s Evolution and Development group, I worked on the mechanical origins of petal epidermal cells patterns.\nUniversity of Cambridge - Sainsbury Laboratory (Cambridge U.K.). Here working with Francois Nèdelec, I carried out investigations of the dynamical origins of order-disorder transitions in the organization of the filaments composing the plant cortex.\nIn all these posts, my duties consist of finding the mechanisms underlying a certain phenomena, this is carried out by the application of the scientific method to postulate a set of hypothesis, abstract these hypothesis into mathematical models, then solve the models and study the solutions to make predictions. I have also carried out work on Bioinformatics at TSL in Norwich U.K. and as a scientific programmer at The European Bioinformatics Institute (EMBL-EBI) in Cambridge. In my spare time I work on robots, data visualisation, computer vision, AI and automation.\n","date":"10 December 2024","externalUrl":null,"permalink":"/experience/resume/","section":"Experiences","summary":"","title":"Resume","type":"experience"},{"content":"","date":"10 December 2024","externalUrl":null,"permalink":"/tags/work-experience/","section":"Tags","summary":"","title":"Work-Experience","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]