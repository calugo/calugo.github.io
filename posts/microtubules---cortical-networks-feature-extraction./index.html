<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Microtubules! - Cortical networks feature extraction. &#183; Carlos's Website</title>
<meta name=title content="Microtubules! - Cortical networks feature extraction. &#183; Carlos's Website"><meta name=description content="Fun and useful analysis of tubulin content"><meta name=keywords content="Data visualization,Microtubules,Feature extraction,Image analysis,"><link rel=canonical href=https://calugo.github.io/posts/microtubules---cortical-networks-feature-extraction./><link type=text/css rel=stylesheet href=/css/main.bundle.min.665e57a6b6a7363e73b154576f2b6058a2fdb0af8120c75b50739753742cd1d7e0a40adb8b79412873c65da39db636d058c97394138a1307523b63c58558a9ff.css integrity="sha512-Zl5XpranNj5zsVRXbytgWKL9sK+BIMdbUHOXU3Qs0dfgpArbi3lBKHPGXaOdtjbQWMlzlBOKEwdSO2PFhVip/w=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.700c68ced6ecd1ce7e26bb8477435b70302e309815080ee5975c308b2030021fb2febf06550bed73e27b5c8feac2526b9061f74d46ff72d939dad11fcabc5963.js integrity="sha512-cAxoztbs0c5+JruEd0NbcDAuMJgVCA7ll1wwiyAwAh+y/r8GVQvtc+J7XI/qwlJrkGH3TUb/ctk52tEfyrxZYw==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://calugo.github.io/posts/microtubules---cortical-networks-feature-extraction./"><meta property="og:site_name" content="Carlos's Website"><meta property="og:title" content="Microtubules! - Cortical networks feature extraction."><meta property="og:description" content="Fun and useful analysis of tubulin content"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-12-27T00:00:00+00:00"><meta property="article:tag" content="Data Visualization"><meta property="article:tag" content="Microtubules"><meta property="article:tag" content="Feature Extraction"><meta property="article:tag" content="Image Analysis"><meta property="og:image" content="https://calugo.github.io/posts/microtubules---cortical-networks-feature-extraction./featured.gif"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://calugo.github.io/posts/microtubules---cortical-networks-feature-extraction./featured.gif"><meta name=twitter:title content="Microtubules! - Cortical networks feature extraction."><meta name=twitter:description content="Fun and useful analysis of tubulin content"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Microtubules! - Cortical networks feature extraction.","headline":"Microtubules! - Cortical networks feature extraction.","description":"Fun and useful analysis of tubulin content","inLanguage":"en","url":"https:\/\/calugo.github.io\/posts\/microtubules---cortical-networks-feature-extraction.\/","author":{"@type":"Person","name":"Carlos A. Lugo Ph.D."},"copyrightYear":"2024","dateCreated":"2024-12-27T00:00:00\u002b00:00","datePublished":"2024-12-27T00:00:00\u002b00:00","dateModified":"2024-12-27T00:00:00\u002b00:00","keywords":["Data visualization","Microtubules","Feature extraction","Image analysis"],"mainEntityOfPage":"true","wordCount":"2095"}]</script><meta name=author content="Carlos A. Lugo Ph.D."><link href=https://www.hackster.io/kupkasmale rel=me><link href=https://github.com/calugo rel=me><link href=https://gitlab.com/calugo rel=me><link href=https://linkedin.com/in/carlos-a-lugo-velez rel=me><link href=https://orcid.org/0000-0003-0095-566X rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script><script defer src=/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><link rel=stylesheet href=/lib/lite-youtube-embed/lite-yt-embed.css integrity><script src=/lib/lite-youtube-embed/lite-yt-embed.58a8a22aed9d1cd05780869df4b0d87972fcd144a9eaff8f9a1453e58e6521c5a89ccd80bdb9f48e25ae5ee87b98bd9de51b14dd9988c60e4c89630433292084.js integrity="sha512-WKiiKu2dHNBXgIad9LDYeXL80USp6v+PmhRT5Y5lIcWonM2Avbn0jiWuXuh7mL2d5RsU3ZmIxg5MiWMEMykghA=="></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Carlos&rsquo;s Website</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Projects</p></a><a href=/about/bio/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title="About me">About</p></a><a href=/experience/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Resume>Experience</p></a><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Projects</p></a></li><li class=mt-1><a href=/about/bio/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title="About me">About</p></a></li><li class=mt-1><a href=/experience/resume/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Resume>Experience</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/posts/microtubules---cortical-networks-feature-extraction./featured_hu14882791739809709287.gif)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Microtubules! - Cortical networks feature extraction.</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><span>2095 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">10 mins</span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><div align=justify><p>In this post I describe a small, yet fun, analysis that might be of use to researchers in the field of microtubules. The goal is to extract estimates of the amount and length of polymer from images of cortical networks such as the one below, courtesy of the <a href=https://sites.wustl.edu/dixitlab/ target=_blank>Ram Dixit lab.</a></p><p>The polymer quantities estimated here can be used to postulate, inform, verify or constrain theoretical, mathematical and computational models of cytokeletal dynamics. Such models can become quite complicated, so, with these estimations are aimed to keep, calibrate or test the models to be within feasible biological, physiological and mechanical limits.</p><p><div id=gallery-777b6003bc7fcf06b68453042868acad class=gallery><img src=gallery/cell.gif class=grid-w100></div><div style=text-align:justify><b><i>Time evolution of cortical microtubule networks in plant hypocotyl cells. This system is an example of a biological complex system out of equilibrium.</i></b></p><p>The networks shown are microscopy images in which the flourescent signal is labeling the tubulin which composes the polymer filaments known as microtubules. The microtubules polymerise and de-polymerise at the tip, and are anchored to the cell membrane, in the inner surface of the cell wall.</p><p>The interaction between microtubules is mechanical due to collision events. An outcome of a collision is that the tip of the <b>incoming</b> tubule can be lost, which results in a rapid depolymerisation or <b>catastrophe</b> of the filament. Other possible scenarios are: the incoming filament simply <b>crosses</b> over the target filament, or the two filaments <b>zip</b> together forming a bundle.</p><p>The frequency of such outcomes, the geometric constraints and some other chemical and mechanical processes determine the organisation of the network. Which can be aligned arrays, like the ones in the movie, or disordered arrays.</p><p>This very lousy introduction to the topic is all I am going to mention, but I really encourage you to consult a proper reference in the subject, such as <a href=https://doi.org/10.1017/CBO9780511607318 target=_blank>this</a>.</p></div><div style=text-align:justify><h2 class="relative group">Extracting the length and amount of polymer using python.<div id=extracting-the-length-and-amount-of-polymer-using-python class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#extracting-the-length-and-amount-of-polymer-using-python aria-label=Anchor>#</a></span></h2><h3 class="relative group">Pre-requisites and libraries.<div id=pre-requisites-and-libraries class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#pre-requisites-and-libraries aria-label=Anchor>#</a></span></h3><p>The main library used is <a href=https://numpy.org/doc/stable/index.html target=_blank>numpy</a></p><p><a href=%22https://pypi.org/project/shapely/%22>Shapely</a> - Very handy geometry package.</p><p><a href=%22https://scikit-image.org/%22>Scikit-image</a> - A very well developed and supported image processing library.</p><p>Matplotlib and pandas for plotting and data processing are also used.</p><div style=text-align:justify><h3 class="relative group">Step 1 - Cell of interest segmentation.<div id=step-1---cell-of-interest-segmentation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-1---cell-of-interest-segmentation aria-label=Anchor>#</a></span></h3><p>We will only be focusing on the cell at the center of the movie. So, the first step is to segment the region of interest across all the frames. This can be easily done by selecting a small set of points outlining the shape of the cell. This can be done using a program such as <a href=%22https://imagej.net/software/fiji/downloads%22>FIJI/ImageJ</a> or the <a href=%22https://gitlab.com/calugo/tubule-tiff-picker>point picker</a> GUI wrote ages ago to annotate microtubule collisions.</p><p>To form a polygon, the first and last points need to be the same. Once you have the set outlining the cell shape, and stored in a file, we need to load these in any way you prefer. Next, we load the tiff file. For this purpose I use scikit-image.</p></div><pre tabindex=0><code>import skimage.io as io
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from shapely.geometry import Polygon, Point
</code></pre><div style=text-align:center><b><i>All the modules required for this post to work.</i></b></div><div style=text-align:justify><p>Assuming that the image stack file is named <code>cortex.tiff</code>, then it can be loaded as follows:</p></div><pre tabindex=0><code>file = &#39;cortex.tiff&#39;;
Z = io.imread(file);
</code></pre><div style=text-align:justify>And that's that! The stack is loaded into a numpy array. By using `Z.shape`, we can obtain information about the movie. In my case `print(Z.shape)` returns the tuple `(200, 512, 512)` which tells us that there are 200 frames, each frame containing a 512x512 array image.<p>To mask each frame, we need to set everything outside the region of interest to zero and keep the inside pixels unaltered.</p></div><div id=gallery-d768a89af3cea40aa25cf3535478b15e class=gallery><img src=gallery/Mask.png class=grid-w100></div><div style=text-align:center><b><i>Step I. Region of interest segmentation. Straightforward, yet essential.</i></b></div><div style=text-align:justify><p>The figure above shows a masked frame. We need to mask all the frames, to do so, first we generate a polygon object and store all the inner points in a list:</p><pre tabindex=0><code>coords=[(i,j) for i,j in zip(list(X),list(Y))]
pl = Polygon(coords)
minx, miny, maxx, maxy = pl.bounds
minx, miny, maxx, maxy = int(minx), int(miny), int(maxx), int(maxy)
box_patch = [[x,y] for x in range(minx,maxx) for y in range(miny,maxy)]
pixels = []
for pb in box_patch: 
    pt = Point(pb[0],pb[1])
    if(pl.contains(pt)):
        pixels.append([int(pb[0]), int(pb[1])]) 
</code></pre><p>The above snippet computes all the pixels inside the <code>shapely</code> polygon object <code>pl</code>. This is assuming that
the polygon point coordinates are stored in the arrays <code>X</code> and <code>Y</code>. Then, to get the masked array, we apply something like the snippet below to every frame:</p><pre tabindex=0><code>A = Z[n,:,:]
B = np.zeros(B.shape)
for pix in pixels:
        B[pix[1],pix[0]]=A[pix[1],pix[0]]
</code></pre><p>By calling <code>pl.area</code>, we get the value of the area enclosed by the polygon, which we will use later to compute the desired length and amount polymer estimates. So far, we have been using pixel units. To translate any results to actual length units, we need the scale conversion factor from the file metadata, In the present example we have \( \lambda = 512 px / 40.96 \mu m = 12.5 \mu m ^{-1} \). The cell area \( A = 93731 px^2 = 600 \mu m ^ {2}\).</p><p>An extra step that can be performed is to scale all the masked frames pixel values to values between zero and one. I did this by applying the following function to the stack <code>Z</code>, using the list of pixels inside the polygon.</p><pre tabindex=0><code>def scaleZ(Z,px):
    
    ZS = np.zeros(Z.shape);
    mins = []; maxs = [];
    
    for k in range(Z.shape[0]):
        for pix in px:
            ZS[k,pix[1],pix[0]] = Z[k,pix[1],pix[0]]
        
    MAX = np.max(ZS)
    ZS = ZS/MAX;
    print(ZS.shape)
    return ZS
</code></pre></div><h3 class="relative group">Step 2 - Background estimation and removal.<div id=step-2---background-estimation-and-removal class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-2---background-estimation-and-removal aria-label=Anchor>#</a></span></h3><div style=text-align:justify><p>We have a stack of images showing the fluorescence signal \( F \) of tubulin. This means the more tubulin, the more intense the signal. If we assume that the variable of interest \( L \) is linearly related to the flourescence, then: \(\ L(r) = \alpha F(r) + \beta \) for every pixel \(r\) in the frame. The task is then to estimate the values of \( \alpha \) and \( \beta \).</p><p>For the case of \( \beta \), if we interpret the masked image as a surface \( z = f(r)\), then the level set \( z = c_b \) with the largest number elements, is precisely the background. This can be found by computing the number of pixels on each level in the range of the signal as follows:</p><pre tabindex=0><code>AVDATX = []; AVDATY = [];

for q in range(Z.shape[0]):
  BA = Z[q,:,:]
  BB = BA[BA &gt; 0]
  nx = np.unique(BB)
  ny = []

  #Computes the level sets
  for nj in nx:
    dyj = BB[BB == nj];
    ny.append(len(dyj))

  # Computes a smoothed 
  # curve from the data

  wnx = []
  wny = []
  dn = 250
  for j in range(0,len(nx)-dn):
    wnx.append(np.mean(nx[j:j+dn]))
    wny.append(np.mean(ny[j:j+dn]))
    
  AVDATX.append(wnx);
  AVDATY.append(wny);

THX=[]
    
# Loops over the smooth versions 
# and finds the pair (xmax,ymax) which 
# maximises the curve

for j in range(ZS.shape[0]):
  ymax = max(AVDATY[j])
  xj = AVDATY[j].index(ymax)
  max = AVDATX[j][xj]
  THX.append(xmax);
</code></pre><p>The video below illustrates the results of what I just described for a single frame.</p><p><lite-youtube videoid=xBFyQNE88gY playlabel=xBFyQNE88gY params="controls=0"></lite-youtube><b><i>Pyvista rendering of a frame as surface. (Left) full frame, (Middle) the masked frame. (Right) The signal with the background substracted.</a></i></b></p><p>Applying the snippet above to every masked frame and collecting every <code>xmax</code> and <code>ymax</code> values, and plotting the results, we get something that looks like:</p><p><div id=gallery-4f922447f5209ef70f767df8036542ec class=gallery><img src=gallery/WEBBKGH5.png class=grid-w100></div><b><i>(Left) Counts of the number of elements on each level set of the masked frames, for every frame. The black plots correspond to the respective smoothed signals using a sliding window, the black lines correspond to the maximum value of each smoothed plot. (Right) The smoothed signals, without the raw level set counts, alongside the set which the maximal numberf of elements (the background).</a></i></b></p><p>From the plots above it es easy to set the background value as the average of values in which the plots are maximised. In this case it give a signal value of around 0.26.</p><p><div id=gallery-2297df6bda827ce5d24a8bc1b9bed5c1 class=gallery><img src=gallery/MTBG.gif class=grid-w100></div><b><i>Results of removing the background in every frame (Right) versus the original data (Left).</a></i></b></p><p>We now need carry out the calibration step to obtain the value of \( \alpha \).</p></div><h3 class="relative group">Step 3 - Tubulin/Intensity ratio estimation.<div id=step-3---tubulinintensity-ratio-estimation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#step-3---tubulinintensity-ratio-estimation aria-label=Anchor>#</a></span></h3><div style=text-align:justify><p>This is the final step. In a nutshell, it consists of picking a segment of what appears to be a <b>single</b> microtubule segment. If the segment has a length \( l_o \) and we measure the amount of signal \( I_o \) within a box of that length and height \( h \). Then we just find the value \( \alpha \) such as \( \ l_o = \alpha I_o \).</p><p>In other words \( \alpha = \frac{l_o}{I_o} \). Therefore, if a given amount of signal \( I \) is measured, we can say that it can be accommodated in a length \( L = \frac{I}{I_o} l_o\). By doing so, we can then calculate the full amount of signal in every frame and thus make an estimation of how long does a filament need to be to accommodate that amount of signal.</p><p>The first thing to do is to navigate through the movie and select a few segments which then will be used to make the calibration. This step is again a step that can be carried out using an external application to save points.</p><p><div id=gallery-8bbb9044a14f6e8ee6f3a5d273add71d class=gallery><img src=gallery/Sl19.png class=grid-w100></div><b><i>Selecting a single filament by marking it allows to work on a small box around the microtubule signal. the sequence above sumarises the steps, until we have a box containing the filament with the background removed.</a></i></b></p><p>If <code>P1 =(X1,Y1)</code> and <code>P2 = (X2,Y2)</code> are the coordinates, defining the segment. I chose to rotate the image around <code>Pa</code> to align the segment with the <code>X</code> axis. This is easily done using the rotate method from scikit image.</p><pre tabindex=0><code>def Box(Zm, X, Y, dn, bkg):      
  X1=X[0]; Y1=Y[0]; X2=X[1];Y2=Y[1];
        
  if Y1 &lt; Y2:
    Xa = X1; Xb = X2
    Ya = Y1; Yb = Y2
  else:
    Xa = X2; Xb = X1;
    Yb = Y1; Ya = Y2;
    
  Ux=Xb-Xa; Uy=Yb-Ya; R=np.sqrt(Ux**2+Uy**2)
  angle=np.arccos(Ux/R); angdeg=angle*(180/np.pi);
 
  Q = rotate(Zm,angdeg,center=(Xa,Ya),
            resize=False,preserve_range=True);

  n1=int(Xa); n2=int(Xa+R)
  m1=int(Ya)-dn; m2=int(Ya)+dn
  
  P = Q[m1:m2,n1:n2]
  Q = P - bkg;
  Q[Q&lt;=0] = 0.0 
        
  return Q
</code></pre><p><b><i>Function to compute box with the marked filament. The arguments are the frame <code>Zm</code>, the filament end points <code>X</code>,<code>Y</code>, the box height <code>dn</code> and the background value <code>bkg</code>.</i></b></p><p>Once we have the box determined, we can systematically vary the width of from zero to values which contains all the signal inside. Then, the criteria to used the amount of signal of a single filament is: the box with width for which the amount of signal start to be almost constant.</p><pre tabindex=0><code>n1 = no
n2 = no+1
P = Q[n1:n2,0:Na]
dn.append(n2-n1)
In.append(sum(P.ravel()))
for n in range(no-1):
  n1 -=1
  n2 += 1          
  P = Q[n1:n2,:]
  dn.append(n2-n1)
  In.append(sum(P.ravel()))
</code></pre><p><b><i>Snippet to compute the signal within a box of height <code>2no</code>. The arguments are <code>no</code> and <code>Na</code> which is the length of the box, as well as the array <code>Q</code>.</i></b></p><p><div id=gallery-361b3327379059b54c2894f17b6226e1 class=gallery><img src=gallery/cal1.png class=grid-w100></div><b><i>Output from performing the calibration step over a segment. (Left) Measures of the amount of signal in the box. (Right) The filament enclosed in boxes of several heights.</i></b></p><p>By choosing several filaments across the movie, we can obtain several points and average to estimate the ratio signal/length.</p><div id=gallery-586e8f6e1ff2d99d959ff3deef634cc4 class=gallery><img src=gallery/CalibrationH5.png class=grid-w100></div><pre tabindex=0><code>#Average of the single segments from the previous step
Sm = avth/(Q.shape[0]*Q.shape[1]) 
#Ratio pixels/length in microns
rj=512/40.96 
#Box length in microns 
dx = Lmin/rj 
#Lists to store the Amount of polymer and length
Lk = [];Ik=[]; 
Area = pl.area

#Area in microns^2
Amu = Area/(rj*rj) 
#Length of equivalent area square
DLA = np.sqrt(Amu)

for j in range(ZS.shape[0]):
    
    Zm = ZS[j,:,:] - bg
    Zm[Zm&lt;=0] = 0.0 
    Q = Zm.ravel()
    Qa = Q[Q&gt;=Sm]
    Qn = sum(Zm.ravel())

    Lk.append((Qn/avth)*dx)
    Ik.append(Qn/Amu)
    
    nmin =  np.min(Qa); nmax = 1
    bins = np.arange(nmin,nmax,0.005)
    yh, bn = np.histogram(Qa, bins=bins)
    
Lkav = np.mean(Lk)
SLkav = np.std(Lk)

Ikav = np.mean(Ik)
SIkav = np.std(Ik)
</code></pre><p>From the previous step we know that \( L = l_o \left( \frac{I}{I_o} \right) \). Whereas the amount of polymer is the sum of the intensity above or equal to the calibration amount \( I_o. \)</p><p>Executing the snippet and collecting the mean values we get an average linear polymer length per area of \( 1.52 \pm 0.13 \mu m \) and the average amount of polymer per cell area of \( 1.73 \pm 0.15 [\mu m ]^{-2} \)</p><div id=gallery-70aa3a8f090da4125255ff55957b8aa0 class=gallery><img src=gallery/H5_Results.png class=grid-w100></div><p>I have prepared a jupyer notebook available here, to be used as you please.</p><p>The full analysis will probably find its way into an article appendix with some luck. If it is not the case, well, at least for now, here it is. To be read and maybe be used by someone interested!</p></div><h3 class="relative group">Useful tools.<div id=useful-tools class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#useful-tools aria-label=Anchor>#</a></span></h3><p><a href=https://gitlab.com/calugo/tubule-tiff-picker>Microtubule Picker - Used in step 1</a></p><p><a href=https://imagej.net/software/fiji/downloads>ImageJ/FIJI - Used in step 1</a></p><p><a href=https://pypi.org/project/shapely/>Shapely - Geometry package</a></p><p><a href=https://scikit-image.org/>Scikit-image - General image processing library</a></p></div></div><script>var oid="views_posts/Microtubules!---Cortical-networks-feature-extraction./index.md",oid_likes="likes_posts/Microtubules!---Cortical-networks-feature-extraction./index.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/the-genetic-code-network/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Interactive network visualisation - The Genetic Code</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"></span></span></a></span><span></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href title>Categories</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Carlos A. Lugo Ph.D.</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer></div></body></html>